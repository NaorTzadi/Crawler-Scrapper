package org.example;
import org.jsoup.Connection;
import org.jsoup.Jsoup;
import org.jsoup.nodes.Document;
import org.jsoup.nodes.Element;
import org.jsoup.select.Elements;
import java.io.IOException;
import java.nio.charset.StandardCharsets;
import java.security.MessageDigest;
import java.security.NoSuchAlgorithmException;
import java.util.*;
import java.util.concurrent.*;
import java.util.concurrent.atomic.AtomicInteger;
public class Crawler {
    private final int THREADS_CAPACITY = Runtime.getRuntime().availableProcessors();
    private final Set<String> visitedLinks = ConcurrentHashMap.newKeySet();
    private final BlockingQueue<String> urlsToCrawl = new LinkedBlockingQueue<>();
    private final ExecutorService threadPool = Executors.newFixedThreadPool(THREADS_CAPACITY);
    private final AtomicInteger iterationsCount = new AtomicInteger(0);
    private boolean shouldStop = false;
    private final String userAgent = Utility.getRandomUserAgent();
    private Map<String, String> cookies = new HashMap<>();
    private String mainDomain;
    private final Set<String> contentHashes = ConcurrentHashMap.newKeySet();
    private final long TIMEOUT_DURATION = TimeUnit.SECONDS.toMillis(5);
    private volatile long lastActivityTime = System.currentTimeMillis();
    private final CountDownLatch latch = new CountDownLatch(THREADS_CAPACITY);
    public WebCrawlerDataBase startCrawling(String url, WebCrawlerDataBase webCrawlerData) {
        //TODO: מה קןרה במצב בו אתר מסויים דורש שימוש רצוף בעוגיות במהלך השימוש
        //TODO: יתכנו תוספות נוספות למחלקה
        //TODO: יש המון בקשות מהאתר בזמן קצר
        mainDomain = Utility.isMainPageUrl(url) ? url : Utility.getMainDomainUrl(url);
        urlsToCrawl.add(url);
        for (int i = 0; i < THREADS_CAPACITY; i++) {
            threadPool.submit(this::crawl);
        }
        threadPool.shutdown();
        try {threadPool.awaitTermination(Long.MAX_VALUE, TimeUnit.NANOSECONDS);
        } catch (InterruptedException e) {Thread.currentThread().interrupt();}
        if (!urlsToCrawl.isEmpty()){System.out.println("something went wrong and we were unable to crawl through all the page urls.");}
        webCrawlerData.setUniqueLinksList(new ArrayList<>(visitedLinks));
        webCrawlerData.setUniqueLinksCount(visitedLinks.size());
        webCrawlerData.setIterationsCount(iterationsCount.get());
        return webCrawlerData;
    }
    private void crawl() {
        try {
            while (!shouldStop && System.currentTimeMillis() - lastActivityTime <= TIMEOUT_DURATION) {
                String url = urlsToCrawl.poll(100, TimeUnit.MILLISECONDS);
                if (url == null) {continue;}
                lastActivityTime = System.currentTimeMillis();
                if (!extractAndHashAndSavePageContent(url)){continue;}
                iterationsCount.incrementAndGet();
                if (visitedLinks.add(url)) {
                    System.out.print("\rNumber of links crawled so far: " + visitedLinks.size());
                    if (visitedLinks.size() % 1500 == 0) {shouldProceed();}
                    urlsToCrawl.addAll(getLinksFromUrl(url));
                }
            }
        }catch (InterruptedException e){
            e.printStackTrace();
        }finally {
            latch.countDown();
        }
    }
    private Set<String> getLinksFromUrl(String url) {
        Set<String> links = new HashSet<>();
        try {
            Connection.Response response = Jsoup.connect(url).userAgent(userAgent).cookies(new HashMap<>(cookies)).execute();
            cookies.putAll(response.cookies());
            Document doc = response.parse();
            Elements linkElements = doc.select("a[href]"); // Select all hyperlinks
            for (Element linkElement : linkElements) {
                String link = linkElement.attr("abs:href"); // Extract absolute URL
                if (isLinkRelatedToTheWebsite(link)&& !visitedLinks.contains(link)) {
                    links.add(link);
                }
            }
        } catch (Exception e) {
            System.err.println("Error fetching links from " + url + ": " + e.getMessage());
        }
        return links;
    }
    private void shouldProceed() {
        Scanner scanner = new Scanner(System.in);
        System.out.println("\nNumber of links crawled: " + iterationsCount.get());
        System.out.println("Do you wish to continue? (yes/no): ");
        String decision = scanner.nextLine().trim().toLowerCase();
        if ("no".equals(decision)) {
            shouldStop = true;
        }
    }
    // אכן פועל אבל לא בטוח עד כמה שימושי משום שלא ידוע לי עדיין הסבירות שיהיו כמה כתובות שונות שמגיעות לאותו העמוד
    private boolean isLinkRelatedToTheWebsite(String link) {
        return !link.isEmpty() && link.contains(mainDomain) && link.length() <= 150;
    }
    private boolean extractAndHashAndSavePageContent(String url){
        return contentHashes.add(migrateExtractedPageContentBytesToHex(Objects.requireNonNull(hashExtractedPageContent(Objects.requireNonNull(extractPageContent(url))))));
    }
    private String extractPageContent(String url){
        try {return Jsoup.connect(url).userAgent(Utility.getRandomUserAgent()).get().text();
        } catch (IOException e) {return null;}
    }
    private byte[] hashExtractedPageContent(String content){
        try {return MessageDigest.getInstance("MD5").digest(content.getBytes(StandardCharsets.UTF_8));
        }catch (NoSuchAlgorithmException e){return null;}
    }
    private String migrateExtractedPageContentBytesToHex(byte[] hash){
        StringBuilder hexString=new StringBuilder(2 * hash.length);
        for (int i = 0; i < hash.length; i++) {
            String hex = Integer.toHexString(0xff & hash[i]);
            if(hex.length() == 1) {
                hexString.append('0');
            }
            hexString.append(hex);
        }
        return hexString.toString();
    }

    /*
    package org.example;
import org.jsoup.Connection;
import org.jsoup.Jsoup;
import org.jsoup.nodes.Document;
import org.jsoup.nodes.Element;
import org.jsoup.select.Elements;
import java.util.*;
public class Crawl {
    private static HashSet<String> visitedLinks= new HashSet<>();
    private static boolean shouldStop=false;
    private static int iterationsCount=0;
    private static Map<String, String> cookies = new HashMap<>();
    private static final String userAgent=Utility.getRandomUserAgent();
    private static String mainDomain;
    public WebCrawlerDataBase startCrawling(String url, WebCrawlerDataBase webCrawlerData){
        // מה קןרה במצב בו אתר מסויים דורש שימוש רצוף בעוגיות במהלך השימוש
        // יתכנו תוספות נוספות למחלקה
        // אפשרויות למנוע מהזחל כניסה לקישורים של מודעות קופצות
        if (!Utility.isMainPageUrl(url)){mainDomain=Utility.getMainDomainUrl(url);}else {mainDomain=url;}
        crawl(url);

        webCrawlerData.setUniqueLinksList(new ArrayList<>(visitedLinks));
        webCrawlerData.setUniqueLinksCount(new ArrayList<>(visitedLinks).size());
        webCrawlerData.setIterationsCount(iterationsCount);
        return webCrawlerData;
    }
    private static void crawl(String url) {
        if(shouldStop){return;}
        iterationsCount++;
        if (visitedLinks.contains(url)) {return;}
        visitedLinks.add(url);
        if (visitedLinks.size() % 1500 == 0) {shouldProceed();}

        System.out.print("\rNumber of links crawled so far: " + visitedLinks.size());
        List<String> links = getLinksFromUrl(url);
        for (String link : links) {
            crawl(link);
        }
    }
    private static List<String> getLinksFromUrl(String url) {
        List<String> links = new ArrayList<>();
        try {
            Connection.Response response = Jsoup.connect(url).userAgent(userAgent).cookies(cookies).execute();
            cookies.putAll(response.cookies());  // Update the stored cookies
            Document doc = response.parse();
            Elements linkElements = doc.select("a[href]");  // Select all hyperlinks
            for (Element linkElement : linkElements) {
                String link = linkElement.attr("abs:href");  // Extract absolute URL
                if (isLinkRelatedToTheWebsite(link)) {
                    links.add(link);
                }
            }
        } catch (Exception e) {
            System.err.println("Error fetching links from " + url + ": " + e.getMessage());
        }
        return links;
    }
    private static void shouldProceed(){
        Scanner scanner=new Scanner(System.in);
        int hasFailedOnce=0;
        final String option1="1"; final String option2="0";
        String decision;
        do {
            if (hasFailedOnce>0){System.out.println("invalid input!!");}
            System.out.println();
            System.out.println("so far we counted: "+visitedLinks.size()+" links.");
            System.out.println("press "+option1+" if you wish to proceed.");
            System.out.println("press "+option2+" if you want to finish.");
            decision=scanner.nextLine();
            hasFailedOnce++;
        }while (!decision.equals(option1)&&!decision.equals(option2));
        shouldStop=decision.equals("0");
        iterationsCount--;
    }
    private static boolean isLinkRelatedToTheWebsite(String link){
        if (link.isEmpty()||(!link.contains(mainDomain) && link.length()>150)){
         return false;   // כנראה חלון קופץ של איזו מודעה כי אינו מכיל את הכתובת הראשית של האתר וארוך מדי
        }
        return true;
    }
  */
}
